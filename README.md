[![Hadoop](https://www.vectorlogo.zone/logos/apache_hadoop/apache_hadoop-ar21.svg)](https://hadoop.apache.org/)
[![ApacheSpark](https://www.vectorlogo.zone/logos/apache_spark/apache_spark-ar21.svg)](https://spark.apache.org/)

# DATA WARE HOUSE - DATA LAKE #

Project about Data

# üìù Menu

* [Install](#markdown-header--nstallation-tool)
    
    * [Hadoop](#hadoop)

    * [Apache Spark](#apache-spark)

    * [Kafka](#)

***

## [![Hadoop](https://www.vectorlogo.zone/logos/apache_hadoop/apache_hadoop-ar21.svg)](https://hadoop.apache.org/)

## üèÅ **C√†i ƒë·∫∑t**

### **C·∫≠p nh·∫≠t h·ªá th·ªëng Ubuntu**

```
sudo apt update && sudo apt upgrade
```

***
### **C√†i ƒë·∫∑t Java Development Kit (JDK)**

```
sudo apt install default-jdk
```

***
### **T·∫°o key cho ng∆∞·ªùi d√πng**

#### T·∫°o key b·∫±ng ph∆∞∆°ng th·ª©c RSA v√† l∆∞u ·ªü .ssh/id_rsa
```
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
```

#### T·∫°o authorized_keys (Key x√°c th·ª±c ng∆∞·ªùi d√πng ·ªü local)
```
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

#### C·∫•p quy·ªÅn cho key ·ªü .ssh/authorized_keys

```
chmod 0600 ~/.ssh/authorized_keys
```

***

### **T·∫£i xu·ªëng Hadoop**

```
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz
```

#### Sau khi t·∫£i xu·ªëng, gi·∫£i n√©n file tar.gz b·∫±ng l·ªánh:

```
tar -xzvf hadoop-3.3.4.tar.gz
```


#### Di chuy·ªÉn th∆∞ m·ª•c Hadoop ƒë√£ gi·∫£i n√©n v√†o th∆∞ m·ª•c /usr/local b·∫±ng l·ªánh:

```
sudo mv hadoop-3.3.4.tar.gz /usr/local/hadoop
```

***

### **Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng cho Hadoop**

#### Tr·ªè v√†o file theo ƒë∆∞·ªùng d·∫´n /etc/environment

```
sudo vi /etc/environment
```

#### Th√™m c√°c d√≤ng sau v√†o t·ªáp tin /etc/environment

```
HADOOP_HOME=/usr/local/hadoop
PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

#### Th·ª±c hi·ªán l·ªánh sau ƒë·ªÉ √°p d·ª•ng c√°c thay ƒë·ªïi cho bi·∫øn m√¥i tr∆∞·ªùng:

```
source /etc/environment
```

***

### **C·∫•u h√¨nh Hadoop**

#### S·ª≠a t·ªáp tin c·∫•u h√¨nh c·ªßa Hadoop (file /usr/local/hadoop/etc/hadoop/hadoop-env.sh) b·∫±ng c√°ch th√™m ƒë∆∞·ªùng d·∫´n c·ªßa JDK v√†o bi·∫øn JAVA_HOME nh∆∞ sau:

```
export JAVA_HOME=/usr/lib/jvm/default-java
```


#### C·∫•u h√¨nh Hadoop b·∫±ng c√°ch ch·ªânh s·ª≠a t·ªáp tin /usr/local/hadoop/etc/hadoop/core-site.xml nh∆∞ sau:
```
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
  </code>
  </pre>
</div>

#### Ch·ªânh s·ª≠a t·ªáp tin /usr/local/hadoop/etc/hadoop/hdfs-site.xml ƒë·ªÉ c·∫•u h√¨nh Hadoop ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu nh∆∞ sau:
```
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;/usr/local/hadoop/hadoop_data/hdfs/namenode&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
    &lt;value&gt;/usr/local/hadoop/hadoop_data/hdfs/datanode&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
```

#### Ch·ªânh s·ª≠a t·ªáp tin /usr/local/hadoop/etc/hadoop/mapred-site.xml ƒë·ªÉ c·∫•u h√¨nh Hadoop ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu nh∆∞ sau:
```
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name>mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
  </code>
  </pre>
</div>

#### Ch·ªânh s·ª≠a t·ªáp tin /usr/local/hadoop/etc/hadoop/yarn-site.xml ƒë·ªÉ c·∫•u h√¨nh Hadoop ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu nh∆∞ sau:
```
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
    &lt;value&gt;127.0.0.1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.acl.enable&lt;/name&gt;
    &lt;value&gt;0&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;&nbsp;&nbsp;&nbsp;
  &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
  </code>
  </pre>
</div>

***

### **ƒê·ªãnh d·∫°ng HDFS nghe m·ªõi kh·ªüi ch·∫°y hadoop l·∫ßn ƒë·∫ßu**

```
hdfs namenode -format
```

***

### **N·∫øu SSH daemon ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t, b·∫°n c√≥ th·ªÉ c√†i ƒë·∫∑t n√≥ b·∫±ng l·ªánh sau:**

```
sudo apt-get install openssh-server
```

#### Sau khi c√†i ƒë·∫∑t xong, ch·∫°y l·ªánh sau ƒë·ªÉ ki·ªÉm tra tr·∫°ng th√°i c·ªßa SSH daemon:

```
sudo service ssh start
```

#### B·∫≠t service ssh t·ª± ch·∫°y khi kh·ªüi ƒë·ªông ubuntu

```
sudo systemctl enable ssh
```

#### Ki·ªÉm tra l·∫°i SSH daemon.

```
sudo service ssh status
```

***

### ***Kh·ªüi ƒë·ªông Hadoop b·∫±ng l·ªánh sau:***
```
$HADOOP_HOME/sbin/start-all.sh
```

#### :smile: Kh·ªüi ch·∫°y ƒë∆∞·ª£c Data Node v√† Name Node trong Hadoop

#### Data Node: http://localhost:9864
#### Name Node: http://localhost:9870

***

## [![Apache Spark](https://www.vectorlogo.zone/logos/apache_spark/apache_spark-ar21.svg)](https://spark.apache.org/)

## üèÅ **C√†i ƒë·∫∑t**

### **C·∫≠p nh·∫≠t h·ªá th·ªëng Ubuntu**

```
sudo apt update && sudo apt upgrade
```

***
### **C√†i ƒë·∫∑t Java Development Kit (JDK)**

```
sudo apt install default-jdk
```

#### **T·∫£i xu·ªëng Apache Spark**

```
wget https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3-scala2.13.tgz
```

#### Sau khi t·∫£i xu·ªëng, gi·∫£i n√©n file tar.gz b·∫±ng l·ªánh:

```
tar -xzvf spark-3.3.2-bin-hadoop3-scala2.13.tgz
```


#### Di chuy·ªÉn th∆∞ m·ª•c Apache Spark ƒë√£ gi·∫£i n√©n v√†o th∆∞ m·ª•c /usr/local b·∫±ng l·ªánh:

```
sudo mv spark-3.3.2-bin-hadoop3-scala2.13 /usr/local/spark
```

***

### **Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng cho Spark**

#### Tr·ªè v√†o file theo ƒë∆∞·ªùng d·∫´n /etc/environment

```
sudo vi /etc/environment
```

#### Th√™m c√°c d√≤ng sau v√†o t·ªáp tin /etc/environment

```
SPARK_HOME=/usr/local/spark
PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```

#### Th·ª±c hi·ªán l·ªánh sau ƒë·ªÉ √°p d·ª•ng c√°c thay ƒë·ªïi cho bi·∫øn m√¥i tr∆∞·ªùng:

```
source /etc/environment
```

***

#### Kh·ªüi ƒë·ªông master spark b·∫±ng l·ªánh sau:
```
$SPARK_HOME/sbin/start-master.sh
```

#### Ki·ªÉm tra Spark ƒë√£ kh·ªüi ch·∫°y: http://172.31.129.237:8080/

***

#### Kh·ªüi ƒë·ªông cluster spark b·∫±ng l·ªánh sau:
```
$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077
```

#### Ki·ªÉm tra Spark ƒë√£ kh·ªüi ch·∫°y: http://172.31.129.237:8080/