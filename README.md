[![Hadoop](https://www.vectorlogo.zone/logos/apache_hadoop/apache_hadoop-ar21.svg)](https://hadoop.apache.org/)
[![ApacheSpark](https://www.vectorlogo.zone/logos/apache_spark/apache_spark-ar21.svg)](https://spark.apache.org/)
[![Kafka](https://www.vectorlogo.zone/logos/apache_kafka/apache_kafka-ar21.svg)](https://kafka.apache.org/)

# DATA WARE HOUSE - DATA LAKE #

Project about Data

# üìù Menu

## [Install](#markdown-header-nstallation-tool)
    
  - [Install general system](#üèÅ-c√†i-ƒë·∫∑t-chung)

  - [Hadoop](#hadoop)

  - [Apache Spark](#apache-spark)

  - [Kafka](#kafka)

***
## üèÅ **C√†i ƒë·∫∑t chung**

### **C·∫≠p nh·∫≠t h·ªá th·ªëng Ubuntu**

```
sudo apt update && sudo apt upgrade
```

***
### **C√†i ƒë·∫∑t Java Development Kit (JDK)**

```
sudo apt install default-jdk
```
### **N·∫øu SSH daemon ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t, b·∫°n c√≥ th·ªÉ c√†i ƒë·∫∑t n√≥ b·∫±ng l·ªánh sau:**

```
sudo apt-get install openssh-server
```

#### Sau khi c√†i ƒë·∫∑t xong, ch·∫°y l·ªánh sau ƒë·ªÉ ki·ªÉm tra SSH daemon:

```
sudo service ssh start
```

#### B·∫≠t service ssh t·ª± ch·∫°y khi kh·ªüi ƒë·ªông ubuntu

```
sudo systemctl enable ssh
```

#### Ki·ªÉm tra l·∫°i tr·∫°ng th√°i SSH daemon.

```
sudo service ssh status
```

***
### **T·∫°o key cho ng∆∞·ªùi d√πng**

#### T·∫°o key b·∫±ng ph∆∞∆°ng th·ª©c RSA v√† l∆∞u ·ªü .ssh/id_rsa
```
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
```

#### T·∫°o authorized_keys (Key x√°c th·ª±c ng∆∞·ªùi d√πng ·ªü local)
```
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

#### C·∫•p quy·ªÅn cho key x√°c th·ª±c ng∆∞·ªùi d√πng

```
chmod 0600 ~/.ssh/authorized_keys
```

***
## [![Hadoop](https://www.vectorlogo.zone/logos/apache_hadoop/apache_hadoop-ar21.svg)](https://hadoop.apache.org/)

## üèÅ **C√†i ƒë·∫∑t Hadoop**
***
### **T·∫£i xu·ªëng Hadoop**

```
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz
```

#### Sau khi t·∫£i xu·ªëng, gi·∫£i n√©n file tar.gz b·∫±ng l·ªánh:

```
tar -xzvf hadoop-3.3.4.tar.gz
```


#### Di chuy·ªÉn th∆∞ m·ª•c Hadoop ƒë√£ gi·∫£i n√©n v√†o th∆∞ m·ª•c /usr/local b·∫±ng l·ªánh:

```
sudo mv hadoop-3.3.4.tar.gz /usr/local/hadoop
```

***

### **Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng cho Hadoop**

#### Tr·ªè v√†o file theo ƒë∆∞·ªùng d·∫´n /etc/environment

```
sudo vi /etc/environment
```

#### Th√™m c√°c d√≤ng sau v√†o t·ªáp tin /etc/environment

```
HADOOP_HOME=/usr/local/hadoop
PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

#### Th·ª±c hi·ªán l·ªánh sau ƒë·ªÉ √°p d·ª•ng c√°c thay ƒë·ªïi cho bi·∫øn m√¥i tr∆∞·ªùng:

```
source /etc/environment
```

***

### **C·∫•u h√¨nh Hadoop**

#### S·ª≠a t·ªáp tin c·∫•u h√¨nh c·ªßa Hadoop (file /usr/local/hadoop/etc/hadoop/hadoop-env.sh) b·∫±ng c√°ch th√™m ƒë∆∞·ªùng d·∫´n c·ªßa JDK v√†o bi·∫øn JAVA_HOME nh∆∞ sau:

```
export JAVA_HOME=/usr/lib/jvm/default-java
```


#### C·∫•u h√¨nh Hadoop b·∫±ng c√°ch ch·ªânh s·ª≠a t·ªáp tin /usr/local/hadoop/etc/hadoop/core-site.xml nh∆∞ sau:

```
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>
```

#### Ch·ªânh s·ª≠a t·ªáp tin /usr/local/hadoop/etc/hadoop/hdfs-site.xml ƒë·ªÉ c·∫•u h√¨nh Hadoop ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu nh∆∞ sau:

```
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/usr/local/hadoop/hadoop_data/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/usr/local/hadoop/hadoop_data/hdfs/datanode</value>
  </property>
</configuration>
```

#### Ch·ªânh s·ª≠a t·ªáp tin /usr/local/hadoop/etc/hadoop/mapred-site.xml ƒë·ªÉ c·∫•u h√¨nh Hadoop ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu nh∆∞ sau:

```
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

#### Ch·ªânh s·ª≠a t·ªáp tin /usr/local/hadoop/etc/hadoop/yarn-site.xml ƒë·ªÉ c·∫•u h√¨nh Hadoop ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu nh∆∞ sau:

```
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>127.0.0.1</value>
  </property>
  <property>
    <name>yarn.acl.enable</name>
    <value>0</value>
  </property>
  <property>
    <name>yarn.nodemanager.env-whitelist</name>&nbsp;&nbsp;&nbsp;
  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
  </property>
</configuration>
```

***

### **ƒê·ªãnh d·∫°ng HDFS nghe m·ªõi kh·ªüi ch·∫°y hadoop l·∫ßn ƒë·∫ßu**

```
hdfs namenode -format
```
***
### ***Kh·ªüi ƒë·ªông Hadoop b·∫±ng l·ªánh sau:***
```
$HADOOP_HOME/sbin/start-all.sh
```

#### :smile: Ki·ªÉm tra Data Node v√† Name Node trong Hadoop

#### Data Node: http://localhost:9864
#### Name Node: http://localhost:9870

***

## [![Apache Spark](https://www.vectorlogo.zone/logos/apache_spark/apache_spark-ar21.svg)](https://spark.apache.org/)

## üèÅ **C√†i ƒë·∫∑t Apache Spark**
***
### **T·∫£i xu·ªëng Apache Spark**

```
wget https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3-scala2.13.tgz
```

#### Sau khi t·∫£i xu·ªëng, gi·∫£i n√©n file tar.gz b·∫±ng l·ªánh:

```
tar -xzvf spark-3.3.2-bin-hadoop3-scala2.13.tgz
```

#### Di chuy·ªÉn th∆∞ m·ª•c Apache Spark ƒë√£ gi·∫£i n√©n v√†o th∆∞ m·ª•c /usr/local b·∫±ng l·ªánh:

```
sudo mv spark-3.3.2-bin-hadoop3-scala2.13 /usr/local/spark
```

***

### **Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng cho Spark**

#### Tr·ªè v√†o file theo ƒë∆∞·ªùng d·∫´n /etc/environment

```
sudo vi /etc/environment
```

#### Th√™m c√°c d√≤ng sau v√†o t·ªáp tin /etc/environment

```
SPARK_HOME=/usr/local/spark
PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```

#### Th·ª±c hi·ªán l·ªánh sau ƒë·ªÉ √°p d·ª•ng c√°c thay ƒë·ªïi cho bi·∫øn m√¥i tr∆∞·ªùng:

```
source /etc/environment
```

***

#### Kh·ªüi ƒë·ªông master spark b·∫±ng l·ªánh sau:
```
$SPARK_HOME/sbin/start-master.sh
```

#### :smile: Ki·ªÉm tra Spark Master: http://172.31.129.237:8080/

***
#### Kh·ªüi ƒë·ªông cluster spark b·∫±ng l·ªánh sau: $SPARK_HOME/sbin/start-worker.sh {{URL}}
#### **{{URL}}: L√† ƒë∆∞·ªùng d·∫´n spark theo master khi start**

<p align="center">
  <a href="" rel="noopener">
 <img src="resource/master_url_spark.png" alt="Project logo"></a>
</p>

```
$SPARK_HOME/sbin/start-worker.sh spark://LAPTOP-HBKNP45N.:7077
```

#### :smile: Ki·ªÉm tra Spark Worker: http://172.31.129.237:8081/

***

## [![Kafka](https://www.vectorlogo.zone/logos/apache_kafka/apache_kafka-ar21.svg)](https://kafka.apache.org/)

## üèÅ **C√†i ƒë·∫∑t Kafka**
***
### **T·∫£i xu·ªëng Kafka**

```
wget https://downloads.apache.org/kafka/3.4.0/kafka_2.12-3.4.0.tgz
```

#### Sau khi t·∫£i xu·ªëng, gi·∫£i n√©n file tar.gz b·∫±ng l·ªánh:

```
tar -xzvf kafka_2.12-3.4.0.tgz --strip 1
```
> --strip 1: ƒê·∫£m b·∫£o khi gi·∫£i n√©n file tgz ƒë·∫£m b·∫£o gi·∫£i n√©n th∆∞ m·ª•c kafka ch·ª© kh√¥ng ph·∫£i th∆∞ m·ª•c con t√™n gi·ªëng kafka b√™n trong n√≥

#### Di chuy·ªÉn th∆∞ m·ª•c kafka ƒë√£ gi·∫£i n√©n v√†o th∆∞ m·ª•c /usr/local b·∫±ng l·ªánh:

```
sudo mv kafka_2.12-3.4.0 /usr/local/kafka
```
***
### **C·∫•u h√¨nh Kafka**

#### **Truy c·∫≠p v√†o file /config/server.properties ·ªü Kafka**

```
sudo vi /usr/local/kafka/config/server.properties
```

#### Kafka topic bao g·ªìm danh m·ª•c, nh√≥m ho·∫∑c t√™n c·ªßa d·ªØ li·ªáu ƒë∆∞·ª£c ƒë√£ ƒë∆∞·ª£c ƒëƒÉng k√Ω. M·∫∑c ƒë·ªãnh ·ªü Kafka s·∫Ω kh√¥ng ƒë∆∞·ª£c x√≥a topic. V√¨ v·∫≠y t·∫Øt ch·ª©c nƒÉng x√≥a topic ·ªü Kafka

```
delete.topic.enable = true
```

#### Ch·ªâ ƒë·ªãnh th∆∞ m·ª•c logs kafka ·ªü local

```
log.dirs=/usr/local/kafka/logs
```


#### T·∫°o serivce Kafka ·ªü Ubuntu bao g·ªìm zookeeper v√† kafka
#### **L∆∞u √Ω: Ch·∫°y service zookeeper tr∆∞·ªõc service kafka**
***
#### **Zookeeper service**

#### **T·∫°o file /etc/systemd/system/zookeeper.service**

```
sudo vi /etc/systemd/system/zookeeper.service
```

#### Ch·ªânh s·ª≠a t·ªáp tin /etc/systemd/system/zookeeper.service
#### **L∆∞u √Ω:**
  > Thay ƒë·ªïi User={{Local User}}. Ki·ªÉm tra {{Local User}} b·∫±ng l·ªánh: whoami

```
[Unit]
Requires=network.target remote-fs.target
After=network.target remote-fs.target

[Service]
Type=simple
User=blue
ExecStart=/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties
ExecStop=/usr/local/kafka/bin/zookeeper-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
```
#### Kh·ªüi ƒë·ªông service zookeeper
  > enable: L√† l·ªánh cho ph√©p d·ªãch v·ª• t·ª± ƒë·ªông kh·ªüi ch·∫°y khi h·ªá th·ªëng ch·∫°y l·∫°i
```
sudo systemctl enable zookeeper.service
```
#### :smile: Ki·ªÉm tra service zookeeper

```
sudo systemctl status zookeeper.service
```

***
#### **Kafka service**

#### **T·∫°o file /etc/systemd/system/kafka.service**

```
sudo vi /etc/systemd/system/kafka.service
```

#### Ch·ªânh s·ª≠a t·ªáp tin /etc/systemd/system/kafka.service
#### **L∆∞u √Ω:** 
  > kafka.log l√† file ƒë∆∞·ª£c kh·ªüi t·∫°o khi run service kafka. Kh√¥ng c√≥ ·ªü m·∫∑c ƒë·ªãnh ·ªü th∆∞ m·ª•c log c·ªßa kafka.
  
  > Thay ƒë·ªïi User={{Local User}}. Ki·ªÉm tra {{Local User}} b·∫±ng l·ªánh: whoami


```
[Unit]
Requires=zookeeper.service
After=zookeeper.service

[Service]
Type=simple
User=blue
ExecStart=/bin/sh -c '/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties > /usr/local/kafka/logs/kafka.log 2>&1'
ExecStop=/usr/local/kafka/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
```

#### Kh·ªüi ƒë·ªông service kafka. 
  > enable: L√† l·ªánh cho ph√©p d·ªãch v·ª• t·ª± ƒë·ªông kh·ªüi ch·∫°y khi h·ªá th·ªëng ch·∫°y l·∫°i

```
sudo systemctl enable kafka.service
```
#### :smile: Ki·ªÉm tra service kafka

```
sudo systemctl status kafka.service
```